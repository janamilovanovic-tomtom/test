// Copyright (C) 2018 TomTom NV. All rights reserved.
//
// This software is the proprietary copyright of TomTom NV and its subsidiaries and may be
// used for internal evaluation purposes or commercial use strictly subject to separate
// license agreement between you and TomTom NV. If you are the licensee, you are only permitted
// to use this software in accordance with the terms of your license agreement. If you are
// not the licensee, you are not authorized to use this software in any manner and should
// immediately return or destroy it.

= Preventing Regressions

== Status

Proposed

== Context

The new instruction engine now correctly handles the majority of
maneuvers in the smoke tests.  This means development is moving into a
new phase where it is more likely that we will break existing
functionality than incorrectly implement new functionality.  We are
already seeing a few regressions.

We should adjust our testing strategy and make sure that we plug the
gaps in a way that is sustainable for the long term.

=== Situation Handlers

A particular problem is the structure of the instruction engine, in
which each situation handler runs in turn until one indicates it can
handle the current junction.  This produces "spooky action at a
distance", where a change to one situation handler which ultimately
means it fails to handle a situation, can lead to bad behaviour from
an entirely different situation handler which is now selected as the
next best alternative.

We have a number of different styles of tests which are covering this
functionality, but each style has problems that mean the coverage is
not as complete as it needs to be.

==== Unit Tests

We have unit tests for each situation handler.  They test that the
situation handler returns the correct instructions when it can handle
the junction, and also test that it returns `boost::none` when it
cannot handle the junction.

A problem with unit tests is that they are written while we implement
features, and each feature is limited to one situation handler that
does actually run.  We therefore test many situations that the handler
can handle, and few situations that the handler cannot handle.  But
correctly failing to handle situations is extremely important
functionality of any given handler.

We have a unit test for the instruction engine itself.  But it does
not test the specific sequence of situation handlers at all.  That is,
currently the roundabout handler is tried before the complex
intersection handler.  If that order were switched, no unit test would
fail.

There is also no test for the overall algorithm, of testing each
situation handler in turn and accepting the first positive result.  If
that logic was changed so that the _last_ positive result was used,
again no unit test would fail.

==== Collaboration Tests

We have a collection of collaboration tests that are more targeted at
ensuring that the correct situation handler is used.  These generally
have a mock map described by a GXL file.  They run the instruction
engine and all the situation handlers, and check that the correct
instructions are issued.

We started out intending to implement a classic test pyramid, in which
the bulk of our tests are unit tests that only test one unit test at a
time, while we have relatively fewer collaboration tests involving
multiple units.  This is because collaboration test are inefficient,
executing more code than needed, and because when they fail it is
often unclear which unit is at fault.  These GXL tests also have the
drawback that because they are relatively realistic, they often
contain more data than is strictly needed to trigger the problem they
are trying to replicate.

However the difficulties with the situation handler model have led to
us relying on collaboration tests more than expected.  We generally
verify that situations are handled correctly by writing a GXL file and
checking the instructions, since otherwise it's hard to prove that a
situation handler will correctly be used.

Now that the bulk of common situations are all being handled
correctly, bugs will more often involve real-world situations that are
more obscure or pathological.  If we continue using collaboration
tests to verify our code, we are in danger of building a huge and
unmanageable library of very specific cases.

==== Smoke Tests

The purpose of smoke tests is to simulate a moderately-sized
collection of real-world cases, and rather than monitor for specific
bad situations, observe the aggregate behaviour for obvious
large-scale unexpected changes.

In practice, we have inherited from NavKit1 the idea that smoke tests
are a collection of origin/destination pairs.  We expect that any
change at all in the generated instructions should be explicitly
acknowledged by committing the changes in smoke test results.

We have not yet incorporated either sense of smoke tests into formal
testing for the new instruction engine.  However there is an ad-hoc
implementation of smoke tests running aside from the main CI system,
which generates aggregate statistics.  The statistics are infrequently
checked for overall progress, and sometimes indicates regressions.

We will probably incorporate a subset of the origin/destination pairs
as a genuine smoke test at some point.  This would use the aggregate
statistics to check for large-scale regressions.  However it is
unlikely to pick up the majority of regressions.  Only the most common
situations will be covered.  And even then, changes in the aggregate
statistics might not be enough to register as a build failure.

A drawback of smoke tests is that because they exist in the real
world, which is constantly changing, the result can be expected to
constantly change as well.  This can be limited by only performing
intermittent map upgrades, but we will inevitably become conditioned
to treat sudden changes in smoke tests as not a serious problem.

We will also gather origin/destination pairs for all the obscure cases
we find during development, with a short note explaining what is
interesting about the situation.  These are not intended for use in
our primary CI system that delivers products to end users.  However
they could be incorporated into a secondary automated build system
that can warn of non-blocking issues.

== Decision

We have so far been unable to reach a definitive decision on our testing strategy.  Instead we are asking developers to be conscious of the problems and try to select the appropriate testing technique for the task they are working on.

* We should invest more effort testing situation handlers in cases
  where they should produce negative results, not only focusing on
  positive results.  For example it is important that the complex
  intersection handler does not run when an arc in the complex
  intersection area is marked as roundabout.  That should be tested.
* We will consider adding additional unit tests for the instruction engine
* We will continue adding collaboration tests
* We will not attempt to cover every edge case with a collaboration test
* We will gather O/D pairs during development
* We will productise a subset of smoke tests for CI

== Consequences

* There is no straightforward procedure for checking against regressions
* We have no way of ensuring that our test coverage is adequate
* Our collection of GXL-based tests will continue to expand
